{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\n\nimport nltk\nfrom nltk.corpus import stopwords\nfrom nltk.stem.snowball import SnowballStemmer\nfrom nltk.tokenize import word_tokenize\nstop_words = set(stopwords.words('english')) \nstemmer = SnowballStemmer(\"english\") \n\nfrom tqdm import tqdm\nimport re\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\n\nimport gensim\nfrom gensim.models import Word2Vec\nfrom gensim.utils import simple_preprocess\nfrom gensim.models.keyedvectors import KeyedVectors\n\nfrom keras.layers import Embedding\nfrom keras import layers\nfrom keras import Input\nfrom keras.models import Model\nfrom keras.optimizers import Adam\nfrom keras.utils import plot_model\nimport tensorflow as tf\nfrom keras.callbacks import EarlyStopping, ModelCheckpoint\nfrom keras import backend as K\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import f1_score, precision_score, recall_score,roc_auc_score,average_precision_score,precision_recall_curve,PrecisionRecallDisplay,roc_curve,RocCurveDisplay","execution_count":9,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"# Load original data \nimport pandas as pd\ntrain_data = pd.read_csv(\"../input/uoe-project-amazon/quora_train.csv\") \nprint(train_data.shape)  \ntrain_data.head(5)\ntest_data = pd.read_csv(\"../input/uoe-project-amazon/quora_test.csv\")  \nprint(test_data.shape)  \ntest_data.head(5)","execution_count":1,"outputs":[{"output_type":"stream","text":"(323480, 7)\n(80871, 7)\n","name":"stdout"},{"output_type":"execute_result","execution_count":1,"data":{"text/plain":"   Unnamed: 0      id    qid1    qid2  \\\n0      384611  384611  752053  752054   \n1      258398  258398  508576  508577   \n2      234932  234932  462847  462848   \n3      347622  347622  681123  681124   \n4      388186  388186  758881  758882   \n\n                                           question1  \\\n0  What are the pros and cons of Legalzoom-genera...   \n1  Why does ready-to-eat poha absorb water instan...   \n2  How apps like paytm earn profit when they are ...   \n3        What daily habits can greatly upgrade life?   \n4   What video game world would you want to live in?   \n\n                                           question2  is_duplicate  \n0             Where can I read reviews of LegalZoom?             0  \n1                        Ultimate teen patti hacker?             0  \n2  How does Paytm earn by giving extra cash back ...             1  \n3  What are your daily habits that improve your p...             1  \n4  If you could live in any video game setting, w...             1  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Unnamed: 0</th>\n      <th>id</th>\n      <th>qid1</th>\n      <th>qid2</th>\n      <th>question1</th>\n      <th>question2</th>\n      <th>is_duplicate</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>384611</td>\n      <td>384611</td>\n      <td>752053</td>\n      <td>752054</td>\n      <td>What are the pros and cons of Legalzoom-genera...</td>\n      <td>Where can I read reviews of LegalZoom?</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>258398</td>\n      <td>258398</td>\n      <td>508576</td>\n      <td>508577</td>\n      <td>Why does ready-to-eat poha absorb water instan...</td>\n      <td>Ultimate teen patti hacker?</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>234932</td>\n      <td>234932</td>\n      <td>462847</td>\n      <td>462848</td>\n      <td>How apps like paytm earn profit when they are ...</td>\n      <td>How does Paytm earn by giving extra cash back ...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>347622</td>\n      <td>347622</td>\n      <td>681123</td>\n      <td>681124</td>\n      <td>What daily habits can greatly upgrade life?</td>\n      <td>What are your daily habits that improve your p...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>388186</td>\n      <td>388186</td>\n      <td>758881</td>\n      <td>758882</td>\n      <td>What video game world would you want to live in?</td>\n      <td>If you could live in any video game setting, w...</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"#check whether data exists missing value and to see the proportion of missing value\ntrain_data.isnull().any() \nprint(train_data.isnull().sum())\n(train_data.shape[0] - train_data.count())/train_data.shape[0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# filling in the blank of nan rows\ntrain_data['question1'].fillna('',inplace=True)\ntrain_data['question2'].fillna('',inplace=True)\ntest_data.fillna('',inplace=True)","execution_count":2,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# double check missing value\ntrain_data.isnull().any() \ntest_data.isnull().any() ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# drop question ID and pairing ID\ntrain_data = train_data.drop(train_data.columns[[0,1,2,3]], axis=1)\ntest_data = test_data.drop(test_data.columns[[0,1,2,3]], axis=1)","execution_count":3,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Initial data analysis"},{"metadata":{"trusted":true},"cell_type":"code","source":"color = sns.color_palette()\ntrain_duplicate = train_data['is_duplicate'].value_counts()\n\nplt.figure(figsize=(6,4))\nsns.barplot(train_duplicate.index, train_duplicate.values, alpha=0.7, color=color[1])\nplt.ylabel('Count', fontsize=12)\nplt.xlabel('Is Duplicate', fontsize=14)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_duplicate / train_duplicate.sum() ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#to check the number of words in a question\nall_question = pd.DataFrame(pd.concat([train_data['question1'], train_data['question2']])) \nall_question.columns = [\"questions\"]\nall_question[\"total_words\"] = all_question[\"questions\"].apply(lambda x : len(str(x).split())) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"count_words = all_question['total_words'].value_counts() \n\nplt.figure(figsize=(12,6))\nsns.barplot(count_words.index, count_words.values, alpha=0.7, color=color[2])\nplt.ylabel('Count', fontsize=12)\nplt.xlabel('Number of words in each question', fontsize=14)\nplt.xticks(rotation='vertical')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Text pre-processing"},{"metadata":{"trusted":true},"cell_type":"code","source":"def decontracted(phrase):\n    '''\n    This function replace abbreviation with the complete words. For example, `won't` would be replaced by `will not`.\n    '''\n    # specific\n    phrase = re.sub(r\"won\\'t\", \"will not\", phrase)\n    phrase = re.sub(r\"can\\'t\", \"can not\", phrase)\n\n    # general\n    phrase = re.sub(r\"n\\'t\", \" not\", phrase)\n    phrase = re.sub(r\"\\'re\", \" are\", phrase)\n    phrase = re.sub(r\"\\'s\", \" is\", phrase)\n    phrase = re.sub(r\"\\'d\", \" would\", phrase)\n    phrase = re.sub(r\"\\'ll\", \" will\", phrase)\n    phrase = re.sub(r\"\\'t\", \" not\", phrase)\n    phrase = re.sub(r\"\\'ve\", \" have\", phrase)\n    phrase = re.sub(r\"\\'m\", \" am\", phrase)\n    return phrase","execution_count":4,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def striphtml(data): \n    '''\n    This function recompile problematic database objects. For example, Avg antivirus 1800</v\\>251<’-‘>4919 Avg tech support phone number 24x7??\n    '''\n    cleanr = re.compile('<.*?>') \n    cleantext = re.sub(cleanr, ' ', str(data)) \n    return cleantext","execution_count":5,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def stripunc(data): \n    return re.sub('[^A-Za-z]+', ' ', str(data), flags=re.MULTILINE|re.DOTALL)","execution_count":6,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def compute(sent): \n    '''\n    This function apply the above three functions and turn uppercase to lowercase.\n    '''\n    sent = decontracted(sent) \n    sent = striphtml(sent) \n    sent = stripunc(sent) \n    words=word_tokenize(str(sent.lower()))   \n    \n    #Removing all single letter and and stopwords from question \n    sent1=' '.join(str(stemmer.stem(j)) for j in words if j not in stop_words and (len(j)!=1)) \n    sent2=' '.join(str(j) for j in words if j not in stop_words and (len(j)!=1)) \n    return sent1, sent2","execution_count":7,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"clean_stemmed_q1 = []\nclean_stemmed_q2 = []\nclean_q1 = []\nclean_q2 = []\ncombined_stemmed_text = []\nfor _, row in tqdm(train_data.iterrows()):\n    csq1, cq1 = compute(row['question1'])\n    csq2, cq2 = compute(row['question2'])\n    clean_stemmed_q1.append(csq1)\n    clean_q1.append(cq1)\n    clean_stemmed_q2.append(csq2)\n    clean_q2.append(cq2)\n    combined_stemmed_text.append(csq1+\" \"+csq2)","execution_count":null,"outputs":[{"output_type":"stream","text":"23450it [00:25, 909.97it/s] ","name":"stderr"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"clean_stemmed_q1_t = []\nclean_stemmed_q2_t = []\nclean_q1_t = []\nclean_q2_t = []\ncombined_stemmed_text_t = []\nfor _, row in tqdm(test_data.iterrows()):\n    \n    csq1_t, cq1_t = compute(row['question1'])\n    csq2_t, cq2_t = compute(row['question2'])\n    clean_stemmed_q1_t.append(csq1_t)\n    clean_q1_t.append(cq1_t)\n    clean_stemmed_q2_t.append(csq2_t)\n    clean_q2_t.append(cq2_t)\n    combined_stemmed_text_t.append(csq1_t+\" \"+csq2_t)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data['clean_stemmed_q1'] = clean_stemmed_q1\ntrain_data['clean_stemmed_q2'] = clean_stemmed_q2\ntrain_data['clean_q1'] = clean_q1\ntrain_data['clean_q2'] = clean_q2\ntrain_data['combined_stemmed_text'] = combined_stemmed_text","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_data['clean_stemmed_q1_t'] = clean_stemmed_q1_t\ntest_data['clean_stemmed_q2_t'] = clean_stemmed_q2_t\ntest_data['clean_q1_t'] = clean_q1_t\ntest_data['clean_q2_t'] = clean_q2_t\ntest_data['combined_stemmed_text_t'] = combined_stemmed_text_t","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#check the format of data\ntrain_data.head()\ntest_data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#split validation dataset\n#clean q1 and q2 datasets\nX_train, X_val, y_train, y_val = train_test_split(train_data[['clean_q1', 'clean_q2']],\n                                                  train_data['is_duplicate'],\n                                                  test_size=0.2, \n                                                  random_state=42) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#stemmed q1 and q2 datasets\nX_train_stemmed, X_val_stemmed = train_test_split(train_data[['clean_stemmed_q1', 'clean_stemmed_q2']],      \n                                                  test_size=0.2, \n                                                  random_state=42) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#combine clean_q1 and clean_q2 horizontally\nX_train['notstemmed_text'] = X_train[['clean_q1','clean_q2']].apply(lambda x:str(x[0])+\" \"+str(x[1]), axis=1) \n\n#combine clean_stemmed_q1 and clean_stemmed_q2 horizontally\nX_train_stemmed['stemmed_text'] = X_train_stemmed[['clean_stemmed_q1','clean_stemmed_q2']].apply(lambda x:str(x[0])+\" \"+str(x[1]), axis=1) ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Tokenizing the data"},{"metadata":{"trusted":true},"cell_type":"code","source":"maxlen = 100 \nmax_words = 10000 \n\n#For not stemmed data(clean_q1 and clean_q2)\ntokenizer_notstemmed = Tokenizer(num_words=max_words)\ntokenizer_notstemmed.fit_on_texts(X_train['notstemmed_text'].values)  \n\nX_train['clean_q1'] = X_train['clean_q1'].astype(str) \nX_train['clean_q2'] = X_train['clean_q2'].astype(str)\nX_val['clean_q1'] = X_val['clean_q1'].astype(str)\nX_val['clean_q2'] = X_val['clean_q2'].astype(str)\n\ntrain_q1_seq = tokenizer_notstemmed.texts_to_sequences(X_train['clean_q1'].values)\ntrain_q2_seq = tokenizer_notstemmed.texts_to_sequences(X_train['clean_q2'].values)\nval_q1_seq = tokenizer_notstemmed.texts_to_sequences(X_val['clean_q1'].values)\nval_q2_seq = tokenizer_notstemmed.texts_to_sequences(X_val['clean_q2'].values)\n\ntrain_q1_seq = pad_sequences(train_q1_seq, maxlen=maxlen, padding='post') \ntrain_q2_seq = pad_sequences(train_q2_seq, maxlen=maxlen, padding='post') \nval_q1_seq = pad_sequences(val_q1_seq, maxlen=maxlen, padding='post') \nval_q2_seq = pad_sequences(val_q2_seq, maxlen=maxlen, padding='post')\n\nword_index_notstemmed = tokenizer_notstemmed.word_index \nprint('Found %s unique tokens for not stemmed words.' % len(word_index_notstemmed))\n\n#For stemmed data\ntokenizer_stemmed = Tokenizer(num_words=max_words)\ntokenizer_stemmed.fit_on_texts(X_train_stemmed['stemmed_text'].values)  \n\n\nX_train_stemmed['clean_stemmed_q1'] = X_train_stemmed['clean_stemmed_q1'].astype(str) \nX_train_stemmed['clean_stemmed_q2'] = X_train_stemmed['clean_stemmed_q2'].astype(str)\nX_val_stemmed['clean_stemmed_q1'] = X_val_stemmed['clean_stemmed_q1'].astype(str)\nX_val_stemmed['clean_stemmed_q2'] = X_val_stemmed['clean_stemmed_q2'].astype(str)\n\ntrain_q1_seq_stemmed = tokenizer_stemmed.texts_to_sequences(X_train_stemmed['clean_stemmed_q1'].values)\ntrain_q2_seq_stemmed = tokenizer_stemmed.texts_to_sequences(X_train_stemmed['clean_stemmed_q2'].values)\nval_q1_seq_stemmed = tokenizer_stemmed.texts_to_sequences(X_val_stemmed['clean_stemmed_q1'].values)\nval_q2_seq_stemmed = tokenizer_stemmed.texts_to_sequences(X_val_stemmed['clean_stemmed_q2'].values)\n\ntrain_q1_seq_stemmed = pad_sequences(train_q1_seq_stemmed, maxlen=maxlen, padding='post') \ntrain_q2_seq_stemmed = pad_sequences(train_q2_seq_stemmed, maxlen=maxlen, padding='post') \nval_q1_seq_stemmed = pad_sequences(val_q1_seq_stemmed, maxlen=maxlen, padding='post') \nval_q2_seq_stemmed = pad_sequences(val_q2_seq_stemmed, maxlen=maxlen, padding='post')\n\nword_index_stemmed = tokenizer_stemmed.word_index \nprint('Found %s unique tokens for stemmed words.' % len(word_index_stemmed))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#same operation to test dataset\ntest_data['clean_q1_t'] = test_data['clean_q1_t'].astype(str)\ntest_data['clean_q2_t'] = test_data['clean_q2_t'].astype(str)\n#not stemmed data\ntest_q1_seq = tokenizer_notstemmed.texts_to_sequences(test_data['clean_q1_t'].values)\ntest_q2_seq = tokenizer_notstemmed.texts_to_sequences(test_data['clean_q2_t'].values)\ntest_q1_seq_stemmed = tokenizer_stemmed.texts_to_sequences(test_data['clean_stemmed_q1_t'].values)\ntest_q2_seq_stemmed = tokenizer_stemmed.texts_to_sequences(test_data['clean_stemmed_q2_t'].values)\n#stemmed data\ntest_q1_seq = pad_sequences(test_q1_seq, maxlen=maxlen, padding='post')\ntest_q2_seq = pad_sequences(test_q2_seq, maxlen=maxlen, padding='post')\ntest_q1_seq_stemmed = pad_sequences(test_q1_seq_stemmed, maxlen=maxlen, padding='post')\ntest_q2_seq_stemmed = pad_sequences(test_q2_seq_stemmed, maxlen=maxlen, padding='post')\n\nlabels_test = np.asarray(test_data['is_duplicate'].to_list())\nprint('Shape of test tensor:', labels_test.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#train and validation dataset for not stemmed q1 and q2 \ntrain_q1_seq\nval_q1_seq\ntrain_q2_seq\nval_q2_seq\n#train and validation dataset for stemmed q1 and q2\ntrain_q1_seq_stemmed\nval_q1_seq_stemmed\ntrain_q2_seq_stemmed\nval_q2_seq_stemmed\n#test dataset for not stemmed q1 and q2\ntest_q1_seq\ntest_q1_seq\n#test dataset for stemmed q1 and q2\ntest_q1_seq_stemmed\ntest_q2_seq_stemmed","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# word2vec embedding"},{"metadata":{"trusted":true},"cell_type":"code","source":"#load in word2vec\nword_vectors = KeyedVectors.load_word2vec_format('../input/nlpword2vecembeddingspretrained/GoogleNews-vectors-negative300.bin', binary=True)\nembedding_dim=300 \nvocabulary_size=min(len(word_index_stemmed)+1,max_words) \n \nembedding_matrix_stemmed = np.zeros((vocabulary_size, embedding_dim))  \n\nfor word, i in word_index_stemmed.items(): \n    if i>=max_words:  \n        continue  \n    try:\n        embedding_vector = word_vectors[word]  \n        embedding_matrix_stemmed[i] = embedding_vector\n    except KeyError:\n        embedding_matrix_stemmed[i]=np.random.normal(0,np.sqrt(0.25),embedding_dim) #？\n\nembedding_matrix_notstemmed = np.zeros((vocabulary_size, embedding_dim)) \nfor word, i in word_index_notstemmed.items(): \n    if i>=max_words:  \n        continue  \n    try:\n        embedding_vector = word_vectors[word] \n        embedding_matrix_notstemmed[i] = embedding_vector\n    except KeyError:\n        embedding_matrix_notstemmed[i]=np.random.normal(0,np.sqrt(0.25),embedding_dim)\n        \ndel(word_vectors)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#check the embedding matrix\nembedding_matrix_stemmed.shape \nembedding_matrix_notstemmed.shape ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class customMetrics(tf.keras.callbacks.Callback):\n    '''\n    This is a custom metric which is accessible using the keras Callback function. \n    This metric saves and prints the F1 score, precision and recall of the validation data after each epoch.\n    The metrics are calculated using functions from `scikit-learn`.\n    '''\n    def __init__(self, validation):   \n        super(customMetrics, self).__init__()\n        self.validation = validation \n        \n    def on_train_begin(self, logs={}):        \n        self.f1_scores = []\n        self.precisions = []\n        self.recalls = []\n        self.auc = []\n        \n    def on_epoch_end(self, epoch, logs={}):\n        true_lebels = self.validation[1]   \n        predictions = (np.asarray(self.model.predict(self.validation[0]))).round()        \n    \n        f1_score_epoch = f1_score(true_lebels, predictions)\n        precision_epoch = precision_score(true_lebels, predictions)\n        recall_epoch = recall_score(true_lebels, predictions)                 \n        auc_epoch = roc_auc_score(true_lebels, predictions)\n        \n        self.f1_scores.append(f1_score_epoch)\n        self.precisions.append(precision_epoch)\n        self.recalls.append(recall_epoch)\n        self.auc.append(auc_epoch)\n        \n        print('For this epoch:\\n','F1 score:', f1_score_epoch, '---Precision:', precision_epoch, '---Recall:', recall_epoch,'---Auc:',auc_epoch)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def loss_plot(history):\n    loss = history.history['loss']\n    val_loss = history.history['val_loss']\n    epochs = range(1, len(loss) + 1)\n    \n    plt.plot(epochs, loss, 'bo', label = 'Training loss')\n    plt.plot(epochs, val_loss, 'b', label = 'Validation loss')\n    plt.title('Training and validation loss')\n    plt.legend()\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"early_stopping = EarlyStopping(monitor='val_loss', patience=3)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Model 1: word2vec embedding+logistic regression"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\ny_training=y_train\ny_training.to_list()\n\n#prepare combined stemmed text for logistic model \ntrain_seq_stemmed = tokenizer_stemmed.texts_to_sequences(X_train_stemmed['stemmed_text'].values)\ntrain_seq_stemmed = pad_sequences(train_seq_stemmed, maxlen=maxlen, padding='post')\ntest_seq_stemmed = tokenizer_stemmed.texts_to_sequences(test_data['combined_stemmed_text_t'].values)\ntest_seq_stemmed = pad_sequences(test_seq_stemmed, maxlen=maxlen, padding='post')\n\n#train:x,y\nlen(y_training)\nlen(train_seq_stemmed)\n#test:x,y\nlen(test_seq_stemmed)\nlen(labels_test)\n\n#fitting logistic model\nlr = LogisticRegression(random_state=0)\nlr.fit(train_seq_stemmed, y_training)\ny_pred=lr.predict(test_seq_stemmed)\nscore = lr.score(train_seq_stemmed, y_training)\nprint(\" mean accuracy on the given test data and labels:\",score) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#using test data to measure the performance of the model\nmodel1_f1 = f1_score(labels_test, y_pred)\nmodel1_precision = precision_score(labels_test, y_pred)    \nmodel1_recall = recall_score(labels_test, y_pred) \nmodel1_auc = roc_auc_score(labels_test, y_pred)\nprint('Test F1 score for Model 1:', model1_f1, '\\nTest precision for Model 1:', model1_precision, \n      '\\nTest recall for Model 1:', model1_recall,'\\nTest auc for Model 1:', model1_auc)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#roc curve\nfpr, tpr, _ = roc_curve(labels_test, y_pred, pos_label=None) \nroc_display = RocCurveDisplay(fpr=fpr, tpr=tpr).plot()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#confusion matrix\nfrom sklearn.metrics import confusion_matrix\nimport matplotlib.pyplot as plt\nconfusion_matrix=confusion_matrix(labels_test,y_pred)\nprint(\"confusion matrix is:\\n\",confusion_matrix)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import confusion_matrix\nconfmat = confusion_matrix(y_true=labels_test, y_pred=y_pred)\nprint(confmat)\n\n# 将混淆矩阵可视化\nfig, ax = plt.subplots(figsize=(2.5, 2.5))\nax.matshow(confmat, cmap=plt.cm.Blues, alpha=0.3)\nfor i in range(confmat.shape[0]):\n    for j in range(confmat.shape[1]):\n        ax.text(x=j, y=i, s=confmat[i, j], va='center', ha='center')\n\nplt.xlabel('预测类标')\nplt.ylabel('真实类标')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#clear memory\nK.clear_session()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Model 2: word2vec embedding+simamese network with LSTM +not trainable"},{"metadata":{"trusted":true},"cell_type":"code","source":"#layer weight sharing (siamese network) (siamese LSTM model/shared LSTM)\ncommon_embed = Embedding(vocabulary_size,embedding_dim,  \n                            weights=[embedding_matrix_notstemmed],     \n                            trainable=False)                 \ncommon_lstm = layers.LSTM(32,name='Siamese_Lstm')    \n\ninput_1 = Input(shape=(train_q1_seq.shape[1],))  \ninput_2 = Input(shape=(train_q2_seq.shape[1],))\n                                                       \nlstm_1=common_embed(input_1)                          \nlstm_2=common_embed(input_2)\nleft_output = common_lstm(lstm_1)                                        \nright_output = common_lstm(lstm_2)  \n\nmerged = layers.concatenate([left_output, right_output], axis = -1,name = 'Concatenation')    \npredictions = layers.Dense(1, activation = 'sigmoid',name = 'Concatenated_Dense')(merged)          \n\n#conncet inputs with the ouputs\nmodel2 = Model([input_1, input_2], predictions) \nmodel2.summary()\nplot_model(model2)\nplot_model(model2,to_file='model2_framwork.png')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"checkpoint2 = ModelCheckpoint(\n    filepath = 'model2.h5',\n    monitor = 'val_loss',\n    save_best_only = True\n)\n\nmodel2.compile(optimizer = 'nadam',\n                loss = 'binary_crossentropy',\n                metrics = [])\n\n\nhistory2= model2.fit([train_q1_seq_stemmed, train_q2_seq_stemmed], \n                        y_train.values.reshape(-1,1),\n                        epochs = 20,\n                        batch_size = 128,\n                        validation_data = ([val_q1_seq_stemmed, val_q2_seq_stemmed], y_val.values.reshape(-1,1)), #23678\n                        callbacks = [customMetrics(validation=([val_q1_seq_stemmed, val_q2_seq_stemmed], y_val.values.reshape(-1,1))),\n                                    checkpoint2,\n                                    early_stopping])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model2_trained = tf.keras.models.load_model('model2.h5')\nmodel2_pred = model2_trained.predict([test_q1_seq_stemmed,test_q2_seq_stemmed]).round()\nmodel2_f1 = f1_score(labels_test, model2_pred)\nmodel2_precision = precision_score(labels_test, model2_pred) \nmodel2_recall = recall_score(labels_test, model2_pred) \nmodel2_auc = roc_auc_score(labels_test, model2_pred)\nprint('Test F1 score for Model 2:', model2_f1, '\\nTest precision for Model 2:', model2_precision, \n      '\\nTest recall for Model 2:', model2_recall,'\\nTest auc for Model 2:', model2_auc)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"loss_plot(history2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#PR curve\nprec, recall, _ = precision_recall_curve(labels_test, model2_pred, #y_test(y_true),y_score\n                                         pos_label=None) \npr_display = PrecisionRecallDisplay(precision=prec, recall=recall).plot()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"aps=average_precision_score(labels_test, model2_pred, average='macro', pos_label=1, sample_weight=None)\nprint(\"Average precision score is:\",aps)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#roc\nfpr, tpr, _ = roc_curve(labels_test, model2_pred, pos_label=None) \nroc_display = RocCurveDisplay(fpr=fpr, tpr=tpr).plot()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Model 3:Bert"},{"metadata":{},"cell_type":"markdown","source":"### prepare data for using Bert"},{"metadata":{"trusted":true},"cell_type":"code","source":"from transformers import BertTokenizer\nimport torch\nfrom torch.utils.data import TensorDataset,random_split\nfrom torch.utils.data import DataLoader,RandomSampler,SequentialSampler\nfrom transformers import BertForSequenceClassification,AdamW,BertConfig\nfrom transformers import get_linear_schedule_with_warmup\nimport time\nimport datetime","execution_count":2,"outputs":[{"output_type":"stream","text":"\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m W&B installed but not logged in.  Run `wandb login` or set the WANDB_API_KEY env variable.\n","name":"stderr"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install transformers","execution_count":3,"outputs":[{"output_type":"stream","text":"Requirement already satisfied: transformers in /opt/conda/lib/python3.7/site-packages (2.11.0)\nRequirement already satisfied: requests in /opt/conda/lib/python3.7/site-packages (from transformers) (2.23.0)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.7/site-packages (from transformers) (2020.4.4)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.7/site-packages (from transformers) (1.18.5)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.7/site-packages (from transformers) (3.0.10)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.7/site-packages (from transformers) (20.1)\nRequirement already satisfied: sentencepiece in /opt/conda/lib/python3.7/site-packages (from transformers) (0.1.91)\nRequirement already satisfied: sacremoses in /opt/conda/lib/python3.7/site-packages (from transformers) (0.0.43)\nRequirement already satisfied: tokenizers==0.7.0 in /opt/conda/lib/python3.7/site-packages (from transformers) (0.7.0)\nRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.7/site-packages (from transformers) (4.45.0)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests->transformers) (2020.6.20)\nRequirement already satisfied: idna<3,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests->transformers) (2.9)\nRequirement already satisfied: chardet<4,>=3.0.2 in /opt/conda/lib/python3.7/site-packages (from requests->transformers) (3.0.4)\nRequirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests->transformers) (1.24.3)\nRequirement already satisfied: six in /opt/conda/lib/python3.7/site-packages (from packaging->transformers) (1.14.0)\nRequirement already satisfied: pyparsing>=2.0.2 in /opt/conda/lib/python3.7/site-packages (from packaging->transformers) (2.4.7)\nRequirement already satisfied: click in /opt/conda/lib/python3.7/site-packages (from sacremoses->transformers) (7.1.1)\nRequirement already satisfied: joblib in /opt/conda/lib/python3.7/site-packages (from sacremoses->transformers) (0.14.1)\n\u001b[33mWARNING: You are using pip version 20.1.1; however, version 20.2 is available.\nYou should consider upgrading via the '/opt/conda/bin/python3.7 -m pip install --upgrade pip' command.\u001b[0m\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"tokenizer=BertTokenizer.from_pretrained('bert-base-uncased',do_lower_case=True)","execution_count":4,"outputs":[{"output_type":"display_data","data":{"text/plain":"HBox(children=(FloatProgress(value=0.0, description='Downloading', max=231508.0, style=ProgressStyle(descripti…","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a362596001ba43ce99b2fe0921f5003f"}},"metadata":{}},{"output_type":"stream","text":"\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"#combine question 1 and question 2 in train and test dataset\ntrain_data['q1_q2_sentences'] = train_data[['question1','question2']].apply(lambda x:str(x[0])+\" \"+str(x[1]), axis=1)\nsentences=train_data.q1_q2_sentences.values\nlabels=train_data.is_duplicate.values","execution_count":5,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"batch_size=32 \ninput_ids=[]\nattention_masks=[]\nfor sent in sentences:\n    encoded_dict=tokenizer.encode_plus(\n    sent,\n    add_special_tokens=True,\n    max_length=100, \n    pad_to_max_length=True,\n    return_attention_mask=True,\n    return_tensors='pt') #pytorch tensor\n    \n    input_ids.append(encoded_dict['input_ids'])\n    attention_masks.append(encoded_dict['attention_mask'])\n    \ninput_ids=torch.cat(input_ids,dim=0) \nattention_masks=torch.cat(attention_masks,dim=0)\nlabels=torch.tensor(labels) \n\nprint('original:',sentences[0])\nprint('token ids:',input_ids[0]) ","execution_count":6,"outputs":[{"output_type":"stream","text":"original: How does banning 500 & 1000 rupee notes solve black money problem? Will the ban on 500 & 1000 rupee notes really work against corruption?\ntoken ids: tensor([  101,  2129,  2515, 21029,  3156,  1004,  6694, 21766, 28084,  3964,\n         9611,  2304,  2769,  3291,  1029,  2097,  1996,  7221,  2006,  3156,\n         1004,  6694, 21766, 28084,  3964,  2428,  2147,  2114,  7897,  1029,\n          102,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0])\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"#training,validation dataset(80%,20%)\n#combine the training inputs into a tensordataset\ndataset=TensorDataset(input_ids,attention_masks,labels) \n\n#spliting\ntrain_size=int(0.8*len(dataset))\nval_size=len(dataset)-train_size\n#spliting dataset\ntrain_dataset,val_dataset=random_split(dataset,[train_size,val_size]) \n\nprint(train_size) \nprint(val_size) ","execution_count":7,"outputs":[{"output_type":"stream","text":"258784\n64696\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_dataloader=DataLoader(\n        train_dataset,\n        sampler=RandomSampler(train_dataset),\n        batch_size=batch_size)\n\nvalidation_dataloader=DataLoader(\n        val_dataset,\n        sampler=RandomSampler(val_dataset),\n        batch_size=batch_size)","execution_count":8,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#train classification model\n#load the pretrained model with a single layer of linear classification layer on top\nmodel = BertForSequenceClassification.from_pretrained(\n    'bert-base-uncased',\n    num_labels=2, #binary classification\n    output_attentions=False,\n    output_hidden_states=False)\nmodel.cuda() #run model on gpu","execution_count":9,"outputs":[{"output_type":"display_data","data":{"text/plain":"HBox(children=(FloatProgress(value=0.0, description='Downloading', max=433.0, style=ProgressStyle(description_…","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c52ff75bd4ea497ca0f70d8600fcd0cc"}},"metadata":{}},{"output_type":"stream","text":"\n","name":"stdout"},{"output_type":"display_data","data":{"text/plain":"HBox(children=(FloatProgress(value=0.0, description='Downloading', max=440473133.0, style=ProgressStyle(descri…","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e21437f4e0254dc696272c94082cf764"}},"metadata":{}},{"output_type":"stream","text":"\n","name":"stdout"},{"output_type":"execute_result","execution_count":9,"data":{"text/plain":"BertForSequenceClassification(\n  (bert): BertModel(\n    (embeddings): BertEmbeddings(\n      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n      (position_embeddings): Embedding(512, 768)\n      (token_type_embeddings): Embedding(2, 768)\n      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n      (dropout): Dropout(p=0.1, inplace=False)\n    )\n    (encoder): BertEncoder(\n      (layer): ModuleList(\n        (0): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (1): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (2): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (3): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (4): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (5): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (6): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (7): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (8): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (9): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (10): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (11): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n    )\n    (pooler): BertPooler(\n      (dense): Linear(in_features=768, out_features=768, bias=True)\n      (activation): Tanh()\n    )\n  )\n  (dropout): Dropout(p=0.1, inplace=False)\n  (classifier): Linear(in_features=768, out_features=2, bias=True)\n)"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"#印参数\nparams=list(model.named_parameters())\nprint(\"The BERT model has {:} different named parameters.\\n\".format(len(params)))\nprint(\"====embedding layer====\\n\")\n\nfor p in params[0:5]:\n    print(\"{:<55}{:>12}\".format(p[0],str(tuple(p[1].size()))))\nprint(\"\\n==== First Transformer ====\\n\")\nfor p in params[5:21]:\n    print(\"{:<55}{:>12}\".format(p[0],str(tuple(p[1].size()))))\nprint(\"\\n==== Output layer ====\\n\")\nfor p in params[-4:]:\n    print(\"{:<55}{:>12}\".format(p[0],str(tuple(p[1].size()))))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Optimize,lR\n#batch size:16,32;,LR:5e-5,3e-5,2e-5;epochs:2,3,4\noptimizer=AdamW(model.parameters(),\n               lr=2e-5,\n               eps=1e-8)","execution_count":10,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"epochs=2 \ntotal_steps=len(train_dataloader)*epochs\nscheduler=get_linear_schedule_with_warmup(optimizer,\n                                          num_warmup_steps=0,\n                                          num_training_steps=total_steps)","execution_count":11,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def flat_accuracy(preds, labels): \n    '''\n    This function to calculate the accuracy of our predictions vs labels\n    '''\n    pred_flat = np.argmax(preds, axis=1).flatten()    \n    labels_flat = labels.flatten()    \n    return np.sum(pred_flat == labels_flat) / len(labels_flat)","execution_count":12,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def format_time(elapsed):\n    elapsed_rounded=int(round(elapsed))\n    return str(datetime.timedelta(seconds=elapsed_rounded))","execution_count":13,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"device_name=tf.test.gpu_device_name()\nif device_name=='/device:GPU:0':\n    print('found')\nelse:\n    print('not')\n    \nif torch.cuda.is_available():\n    device=torch.device('cuda')\n    print(\"there are %d gpu available\"% torch.cuda.device_count())\nelse:\n    print(\"no\")\n    device=torch.device(\"cpu\")","execution_count":16,"outputs":[{"output_type":"stream","text":"found\nthere are 1 gpu available\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Training loop\nimport random\nseed_val=42\nrandom.seed(seed_val)\nnp.random.seed(seed_val)\ntorch.manual_seed(seed_val)\ntorch.cuda.manual_seed_all(seed_val)\n\ntraining_stats = []\ntotal_t0=time.time()\nfor epoch_i in range(0,epochs):\n    #training\n    print(\"\")\n    print(\"--epoch{:}/{:}----\".format(epoch_i+1,epochs))\n    print(\"training...\")\n    \n    t0=time.time()\n    total_train_loss=0\n    model.train() #put the model in 'training' mode\n    for step,batch in enumerate(train_dataloader):\n        #progress update every 40 batches\n        if step % 40 ==0 and not step == 0:\n            elapsed=format_time(time.time()-t0)\n            \n            print(\"batch{:>5,} of {:>5}.  elapsed: {:}.\".format(step,len(train_dataloader),elapsed))\n        #unpack training batch from dataloader\n        #gpu:'to' method\n        b_input_ids=batch[0].to(device)\n        b_input_mask=batch[1].to(device)\n        b_labels=batch[2].to(device)\n        \n        #remove gradients before backward pass\n        model.zero_grad()\n        \n        #forward pass\n        loss,logits=model(b_input_ids,\n                         token_type_ids=None,\n                         attention_mask=b_input_mask,\n                         labels=b_labels)\n        #accumulate the training loss to compute average loss at the end \n        total_train_loss += loss.item()\n        \n        #backward pass to calculate the gradients\n        loss.backward()\n        \n        #deal with gradients to prevent 'exploding gradients'\n        torch.nn.utils.clip_grad_norm_(model.parameters(),1.0)\n        \n        #update parameter and take a step by using the computed gradient\n        #optimizer:difine how parameter are modified on their gradients,and LR\n        optimizer.step()\n        #update the LR\n        scheduler.step()\n    \n    #compute the average loss\n    avg_train_loss = total_train_loss / len(train_dataloader)\n    \n    #measure how long epoch took\n    training_time = format_time(time.time()-t0)\n    \n    print(\"\")\n    print(\"average training loss:{0:.2f}\".format(avg_train_loss))\n    print(\"training epoch took:{:}\".format(training_time))\n    \n    #=====validation\n    print(\"\")\n    print(\"running validation...\")\n    \n    t0=time.time()\n    \n    #put the model in 'validation' mode\n    model.eval()\n    \n    #tracking variables\n    total_eval_accuracy = 0\n    total_eval_loss = 0\n    nb_eval_steps = 0\n    \n    for batch in validation_dataloader:\n        #gpu!\n        b_input_ids=batch[0].to(device)\n        b_input_mask=batch[1].to(device)\n        b_labels=batch[2].to(device)\n        \n        with torch.no_grad():\n            #token_type_ids = 'segment ids'\n            #logits are the output\n            loss,logits=model(b_input_ids,\n                         token_type_ids=None,\n                         attention_mask=b_input_mask,\n                         labels=b_labels)\n        #accumulate the validation loss\n        total_eval_loss += loss.item()\n        \n        #move logtis and labels to gpu \n        logits=logits.detach().cpu().numpy()\n        label_ids = b_labels.to('cpu').numpy()\n        \n        #accmulate the accuracy\n        total_eval_accuracy += flat_accuracy(logits,label_ids)\n        \n    #report final accuracy for validation\n    avg_val_accuracy = total_eval_accuracy / len(validation_dataloader)\n    print(\"accuracy:{0:.2f}\".format(avg_val_accuracy))\n    \n    avg_val_loss = total_eval_loss / len(validation_dataloader)\n    \n    validation_time = format_time(time.time()-t0)\n    print(\"average validation loss:{0:.2f}\".format(avg_val_loss))\n    print(\"validation took:{:}\".format(validation_time))\n    \n    #record all statistics from this epoch\n    training_stats.append(\n        {\n        'epoch': epoch_i+1,\n        'training loss': avg_train_loss,\n        'valid.loss': avg_val_loss,\n        'valid accur.': avg_val_accuracy,\n        'training time': training_time,\n        'validation time': validation_time\n        }\n    )\n    \nprint(\"\")\nprint(\"training complete!\")\nprint(\"total training took {:} (h:mm:ss)\".format(format_time(time.time()-total_t0)))","execution_count":17,"outputs":[{"output_type":"stream","text":"\n--epoch1/2----\ntraining...\nbatch   40 of  8087.  elapsed: 0:00:13.\nbatch   80 of  8087.  elapsed: 0:00:26.\nbatch  120 of  8087.  elapsed: 0:00:38.\nbatch  160 of  8087.  elapsed: 0:00:51.\nbatch  200 of  8087.  elapsed: 0:01:03.\nbatch  240 of  8087.  elapsed: 0:01:16.\nbatch  280 of  8087.  elapsed: 0:01:28.\nbatch  320 of  8087.  elapsed: 0:01:41.\nbatch  360 of  8087.  elapsed: 0:01:53.\nbatch  400 of  8087.  elapsed: 0:02:06.\nbatch  440 of  8087.  elapsed: 0:02:18.\nbatch  480 of  8087.  elapsed: 0:02:31.\nbatch  520 of  8087.  elapsed: 0:02:44.\nbatch  560 of  8087.  elapsed: 0:02:56.\nbatch  600 of  8087.  elapsed: 0:03:09.\nbatch  640 of  8087.  elapsed: 0:03:21.\nbatch  680 of  8087.  elapsed: 0:03:34.\nbatch  720 of  8087.  elapsed: 0:03:46.\nbatch  760 of  8087.  elapsed: 0:03:59.\nbatch  800 of  8087.  elapsed: 0:04:11.\nbatch  840 of  8087.  elapsed: 0:04:24.\nbatch  880 of  8087.  elapsed: 0:04:36.\nbatch  920 of  8087.  elapsed: 0:04:49.\nbatch  960 of  8087.  elapsed: 0:05:01.\nbatch1,000 of  8087.  elapsed: 0:05:14.\nbatch1,040 of  8087.  elapsed: 0:05:27.\nbatch1,080 of  8087.  elapsed: 0:05:39.\nbatch1,120 of  8087.  elapsed: 0:05:52.\nbatch1,160 of  8087.  elapsed: 0:06:04.\nbatch1,200 of  8087.  elapsed: 0:06:16.\nbatch1,240 of  8087.  elapsed: 0:06:29.\nbatch1,280 of  8087.  elapsed: 0:06:42.\nbatch1,320 of  8087.  elapsed: 0:06:54.\nbatch1,360 of  8087.  elapsed: 0:07:07.\nbatch1,400 of  8087.  elapsed: 0:07:19.\nbatch1,440 of  8087.  elapsed: 0:07:32.\nbatch1,480 of  8087.  elapsed: 0:07:44.\nbatch1,520 of  8087.  elapsed: 0:07:57.\nbatch1,560 of  8087.  elapsed: 0:08:09.\nbatch1,600 of  8087.  elapsed: 0:08:22.\nbatch1,640 of  8087.  elapsed: 0:08:34.\nbatch1,680 of  8087.  elapsed: 0:08:47.\nbatch1,720 of  8087.  elapsed: 0:08:59.\nbatch1,760 of  8087.  elapsed: 0:09:12.\nbatch1,800 of  8087.  elapsed: 0:09:24.\nbatch1,840 of  8087.  elapsed: 0:09:37.\nbatch1,880 of  8087.  elapsed: 0:09:49.\nbatch1,920 of  8087.  elapsed: 0:10:02.\nbatch1,960 of  8087.  elapsed: 0:10:14.\nbatch2,000 of  8087.  elapsed: 0:10:27.\nbatch2,040 of  8087.  elapsed: 0:10:40.\nbatch2,080 of  8087.  elapsed: 0:10:52.\nbatch2,120 of  8087.  elapsed: 0:11:04.\nbatch2,160 of  8087.  elapsed: 0:11:17.\nbatch2,200 of  8087.  elapsed: 0:11:30.\nbatch2,240 of  8087.  elapsed: 0:11:42.\nbatch2,280 of  8087.  elapsed: 0:11:55.\nbatch2,320 of  8087.  elapsed: 0:12:07.\nbatch2,360 of  8087.  elapsed: 0:12:20.\nbatch2,400 of  8087.  elapsed: 0:12:32.\nbatch2,440 of  8087.  elapsed: 0:12:45.\nbatch2,480 of  8087.  elapsed: 0:12:57.\nbatch2,520 of  8087.  elapsed: 0:13:10.\nbatch2,560 of  8087.  elapsed: 0:13:22.\nbatch2,600 of  8087.  elapsed: 0:13:35.\nbatch2,640 of  8087.  elapsed: 0:13:47.\nbatch2,680 of  8087.  elapsed: 0:14:00.\nbatch2,720 of  8087.  elapsed: 0:14:12.\nbatch2,760 of  8087.  elapsed: 0:14:25.\nbatch2,800 of  8087.  elapsed: 0:14:37.\nbatch2,840 of  8087.  elapsed: 0:14:50.\nbatch2,880 of  8087.  elapsed: 0:15:02.\nbatch2,920 of  8087.  elapsed: 0:15:15.\nbatch2,960 of  8087.  elapsed: 0:15:27.\nbatch3,000 of  8087.  elapsed: 0:15:40.\nbatch3,040 of  8087.  elapsed: 0:15:52.\nbatch3,080 of  8087.  elapsed: 0:16:05.\nbatch3,120 of  8087.  elapsed: 0:16:18.\nbatch3,160 of  8087.  elapsed: 0:16:30.\nbatch3,200 of  8087.  elapsed: 0:16:43.\nbatch3,240 of  8087.  elapsed: 0:16:55.\nbatch3,280 of  8087.  elapsed: 0:17:08.\nbatch3,320 of  8087.  elapsed: 0:17:20.\nbatch3,360 of  8087.  elapsed: 0:17:33.\nbatch3,400 of  8087.  elapsed: 0:17:45.\nbatch3,440 of  8087.  elapsed: 0:17:58.\nbatch3,480 of  8087.  elapsed: 0:18:10.\nbatch3,520 of  8087.  elapsed: 0:18:23.\nbatch3,560 of  8087.  elapsed: 0:18:35.\nbatch3,600 of  8087.  elapsed: 0:18:48.\nbatch3,640 of  8087.  elapsed: 0:19:00.\nbatch3,680 of  8087.  elapsed: 0:19:13.\nbatch3,720 of  8087.  elapsed: 0:19:26.\nbatch3,760 of  8087.  elapsed: 0:19:38.\nbatch3,800 of  8087.  elapsed: 0:19:51.\nbatch3,840 of  8087.  elapsed: 0:20:03.\nbatch3,880 of  8087.  elapsed: 0:20:15.\nbatch3,920 of  8087.  elapsed: 0:20:28.\nbatch3,960 of  8087.  elapsed: 0:20:41.\nbatch4,000 of  8087.  elapsed: 0:20:53.\nbatch4,040 of  8087.  elapsed: 0:21:06.\nbatch4,080 of  8087.  elapsed: 0:21:18.\nbatch4,120 of  8087.  elapsed: 0:21:31.\nbatch4,160 of  8087.  elapsed: 0:21:43.\nbatch4,200 of  8087.  elapsed: 0:21:56.\nbatch4,240 of  8087.  elapsed: 0:22:08.\nbatch4,280 of  8087.  elapsed: 0:22:21.\nbatch4,320 of  8087.  elapsed: 0:22:33.\nbatch4,360 of  8087.  elapsed: 0:22:46.\nbatch4,400 of  8087.  elapsed: 0:22:58.\nbatch4,440 of  8087.  elapsed: 0:23:11.\nbatch4,480 of  8087.  elapsed: 0:23:24.\nbatch4,520 of  8087.  elapsed: 0:23:36.\nbatch4,560 of  8087.  elapsed: 0:23:49.\nbatch4,600 of  8087.  elapsed: 0:24:01.\nbatch4,640 of  8087.  elapsed: 0:24:14.\nbatch4,680 of  8087.  elapsed: 0:24:26.\nbatch4,720 of  8087.  elapsed: 0:24:39.\nbatch4,760 of  8087.  elapsed: 0:24:51.\nbatch4,800 of  8087.  elapsed: 0:25:04.\nbatch4,840 of  8087.  elapsed: 0:25:16.\nbatch4,880 of  8087.  elapsed: 0:25:29.\nbatch4,920 of  8087.  elapsed: 0:25:42.\nbatch4,960 of  8087.  elapsed: 0:25:54.\nbatch5,000 of  8087.  elapsed: 0:26:07.\nbatch5,040 of  8087.  elapsed: 0:26:19.\nbatch5,080 of  8087.  elapsed: 0:26:32.\nbatch5,120 of  8087.  elapsed: 0:26:45.\nbatch5,160 of  8087.  elapsed: 0:26:57.\nbatch5,200 of  8087.  elapsed: 0:27:10.\nbatch5,240 of  8087.  elapsed: 0:27:22.\nbatch5,280 of  8087.  elapsed: 0:27:35.\nbatch5,320 of  8087.  elapsed: 0:27:47.\nbatch5,360 of  8087.  elapsed: 0:28:00.\nbatch5,400 of  8087.  elapsed: 0:28:12.\nbatch5,440 of  8087.  elapsed: 0:28:25.\nbatch5,480 of  8087.  elapsed: 0:28:37.\nbatch5,520 of  8087.  elapsed: 0:28:50.\nbatch5,560 of  8087.  elapsed: 0:29:02.\nbatch5,600 of  8087.  elapsed: 0:29:15.\nbatch5,640 of  8087.  elapsed: 0:29:27.\nbatch5,680 of  8087.  elapsed: 0:29:40.\nbatch5,720 of  8087.  elapsed: 0:29:52.\nbatch5,760 of  8087.  elapsed: 0:30:05.\nbatch5,800 of  8087.  elapsed: 0:30:18.\nbatch5,840 of  8087.  elapsed: 0:30:30.\nbatch5,880 of  8087.  elapsed: 0:30:43.\nbatch5,920 of  8087.  elapsed: 0:30:55.\nbatch5,960 of  8087.  elapsed: 0:31:08.\nbatch6,000 of  8087.  elapsed: 0:31:20.\nbatch6,040 of  8087.  elapsed: 0:31:33.\nbatch6,080 of  8087.  elapsed: 0:31:45.\nbatch6,120 of  8087.  elapsed: 0:31:58.\nbatch6,160 of  8087.  elapsed: 0:32:10.\nbatch6,200 of  8087.  elapsed: 0:32:23.\nbatch6,240 of  8087.  elapsed: 0:32:36.\nbatch6,280 of  8087.  elapsed: 0:32:48.\nbatch6,320 of  8087.  elapsed: 0:33:01.\nbatch6,360 of  8087.  elapsed: 0:33:13.\nbatch6,400 of  8087.  elapsed: 0:33:26.\nbatch6,440 of  8087.  elapsed: 0:33:38.\nbatch6,480 of  8087.  elapsed: 0:33:51.\nbatch6,520 of  8087.  elapsed: 0:34:03.\nbatch6,560 of  8087.  elapsed: 0:34:16.\nbatch6,600 of  8087.  elapsed: 0:34:28.\nbatch6,640 of  8087.  elapsed: 0:34:41.\nbatch6,680 of  8087.  elapsed: 0:34:53.\nbatch6,720 of  8087.  elapsed: 0:35:06.\nbatch6,760 of  8087.  elapsed: 0:35:18.\nbatch6,800 of  8087.  elapsed: 0:35:31.\nbatch6,840 of  8087.  elapsed: 0:35:43.\nbatch6,880 of  8087.  elapsed: 0:35:56.\nbatch6,920 of  8087.  elapsed: 0:36:08.\nbatch6,960 of  8087.  elapsed: 0:36:21.\nbatch7,000 of  8087.  elapsed: 0:36:34.\nbatch7,040 of  8087.  elapsed: 0:36:46.\nbatch7,080 of  8087.  elapsed: 0:36:58.\nbatch7,120 of  8087.  elapsed: 0:37:11.\nbatch7,160 of  8087.  elapsed: 0:37:24.\nbatch7,200 of  8087.  elapsed: 0:37:36.\nbatch7,240 of  8087.  elapsed: 0:37:49.\nbatch7,280 of  8087.  elapsed: 0:38:01.\nbatch7,320 of  8087.  elapsed: 0:38:14.\nbatch7,360 of  8087.  elapsed: 0:38:26.\nbatch7,400 of  8087.  elapsed: 0:38:39.\nbatch7,440 of  8087.  elapsed: 0:38:51.\nbatch7,480 of  8087.  elapsed: 0:39:04.\nbatch7,520 of  8087.  elapsed: 0:39:16.\nbatch7,560 of  8087.  elapsed: 0:39:29.\nbatch7,600 of  8087.  elapsed: 0:39:41.\nbatch7,640 of  8087.  elapsed: 0:39:54.\nbatch7,680 of  8087.  elapsed: 0:40:06.\nbatch7,720 of  8087.  elapsed: 0:40:19.\nbatch7,760 of  8087.  elapsed: 0:40:31.\nbatch7,800 of  8087.  elapsed: 0:40:44.\nbatch7,840 of  8087.  elapsed: 0:40:56.\nbatch7,880 of  8087.  elapsed: 0:41:09.\nbatch7,920 of  8087.  elapsed: 0:41:22.\nbatch7,960 of  8087.  elapsed: 0:41:34.\nbatch8,000 of  8087.  elapsed: 0:41:47.\nbatch8,040 of  8087.  elapsed: 0:41:59.\nbatch8,080 of  8087.  elapsed: 0:42:12.\n\naverage training loss:0.31\ntraining epoch took:0:42:14\n\nrunning validation...\naccuracy:0.89\naverage validation loss:0.25\nvalidation took:0:03:10\n\n--epoch2/2----\ntraining...\n","name":"stdout"},{"output_type":"stream","text":"batch   40 of  8087.  elapsed: 0:00:12.\nbatch   80 of  8087.  elapsed: 0:00:25.\nbatch  120 of  8087.  elapsed: 0:00:37.\nbatch  160 of  8087.  elapsed: 0:00:50.\nbatch  200 of  8087.  elapsed: 0:01:03.\nbatch  240 of  8087.  elapsed: 0:01:15.\nbatch  280 of  8087.  elapsed: 0:01:27.\nbatch  320 of  8087.  elapsed: 0:01:40.\nbatch  360 of  8087.  elapsed: 0:01:52.\nbatch  400 of  8087.  elapsed: 0:02:05.\nbatch  440 of  8087.  elapsed: 0:02:18.\nbatch  480 of  8087.  elapsed: 0:02:30.\nbatch  520 of  8087.  elapsed: 0:02:43.\nbatch  560 of  8087.  elapsed: 0:02:55.\nbatch  600 of  8087.  elapsed: 0:03:08.\nbatch  640 of  8087.  elapsed: 0:03:20.\nbatch  680 of  8087.  elapsed: 0:03:33.\nbatch  720 of  8087.  elapsed: 0:03:45.\nbatch  760 of  8087.  elapsed: 0:03:58.\nbatch  800 of  8087.  elapsed: 0:04:10.\nbatch  840 of  8087.  elapsed: 0:04:23.\nbatch  880 of  8087.  elapsed: 0:04:35.\nbatch  920 of  8087.  elapsed: 0:04:48.\nbatch  960 of  8087.  elapsed: 0:05:00.\nbatch1,000 of  8087.  elapsed: 0:05:13.\nbatch1,040 of  8087.  elapsed: 0:05:25.\nbatch1,080 of  8087.  elapsed: 0:05:38.\nbatch1,120 of  8087.  elapsed: 0:05:50.\nbatch1,160 of  8087.  elapsed: 0:06:03.\nbatch1,200 of  8087.  elapsed: 0:06:15.\nbatch1,240 of  8087.  elapsed: 0:06:28.\nbatch1,280 of  8087.  elapsed: 0:06:40.\nbatch1,320 of  8087.  elapsed: 0:06:53.\nbatch1,360 of  8087.  elapsed: 0:07:05.\nbatch1,400 of  8087.  elapsed: 0:07:18.\nbatch1,440 of  8087.  elapsed: 0:07:30.\nbatch1,480 of  8087.  elapsed: 0:07:43.\nbatch1,520 of  8087.  elapsed: 0:07:55.\nbatch1,560 of  8087.  elapsed: 0:08:08.\nbatch1,600 of  8087.  elapsed: 0:08:20.\nbatch1,640 of  8087.  elapsed: 0:08:33.\nbatch1,680 of  8087.  elapsed: 0:08:45.\nbatch1,720 of  8087.  elapsed: 0:08:58.\nbatch1,760 of  8087.  elapsed: 0:09:10.\nbatch1,800 of  8087.  elapsed: 0:09:23.\nbatch1,840 of  8087.  elapsed: 0:09:35.\nbatch1,880 of  8087.  elapsed: 0:09:48.\nbatch1,920 of  8087.  elapsed: 0:10:00.\nbatch1,960 of  8087.  elapsed: 0:10:13.\nbatch2,000 of  8087.  elapsed: 0:10:25.\nbatch2,040 of  8087.  elapsed: 0:10:38.\nbatch2,080 of  8087.  elapsed: 0:10:50.\nbatch2,120 of  8087.  elapsed: 0:11:03.\nbatch2,160 of  8087.  elapsed: 0:11:15.\nbatch2,200 of  8087.  elapsed: 0:11:28.\nbatch2,240 of  8087.  elapsed: 0:11:40.\nbatch2,280 of  8087.  elapsed: 0:11:53.\nbatch2,320 of  8087.  elapsed: 0:12:06.\nbatch2,360 of  8087.  elapsed: 0:12:18.\nbatch2,400 of  8087.  elapsed: 0:12:30.\nbatch2,440 of  8087.  elapsed: 0:12:43.\nbatch2,480 of  8087.  elapsed: 0:12:56.\nbatch2,520 of  8087.  elapsed: 0:13:08.\nbatch2,560 of  8087.  elapsed: 0:13:21.\nbatch2,600 of  8087.  elapsed: 0:13:33.\nbatch2,640 of  8087.  elapsed: 0:13:46.\nbatch2,680 of  8087.  elapsed: 0:13:58.\nbatch2,720 of  8087.  elapsed: 0:14:11.\nbatch2,760 of  8087.  elapsed: 0:14:23.\nbatch2,800 of  8087.  elapsed: 0:14:36.\nbatch2,840 of  8087.  elapsed: 0:14:48.\nbatch2,880 of  8087.  elapsed: 0:15:01.\nbatch2,920 of  8087.  elapsed: 0:15:14.\nbatch2,960 of  8087.  elapsed: 0:15:26.\nbatch3,000 of  8087.  elapsed: 0:15:39.\nbatch3,040 of  8087.  elapsed: 0:15:51.\nbatch3,080 of  8087.  elapsed: 0:16:04.\nbatch3,120 of  8087.  elapsed: 0:16:16.\nbatch3,160 of  8087.  elapsed: 0:16:29.\nbatch3,200 of  8087.  elapsed: 0:16:41.\nbatch3,240 of  8087.  elapsed: 0:16:54.\nbatch3,280 of  8087.  elapsed: 0:17:06.\nbatch3,320 of  8087.  elapsed: 0:17:19.\nbatch3,360 of  8087.  elapsed: 0:17:31.\nbatch3,400 of  8087.  elapsed: 0:17:44.\nbatch3,440 of  8087.  elapsed: 0:17:56.\nbatch3,480 of  8087.  elapsed: 0:18:09.\nbatch3,520 of  8087.  elapsed: 0:18:21.\nbatch3,560 of  8087.  elapsed: 0:18:34.\nbatch3,600 of  8087.  elapsed: 0:18:46.\nbatch3,640 of  8087.  elapsed: 0:18:59.\nbatch3,680 of  8087.  elapsed: 0:19:11.\nbatch3,720 of  8087.  elapsed: 0:19:24.\nbatch3,760 of  8087.  elapsed: 0:19:36.\nbatch3,800 of  8087.  elapsed: 0:19:49.\nbatch3,840 of  8087.  elapsed: 0:20:01.\nbatch3,880 of  8087.  elapsed: 0:20:14.\nbatch3,920 of  8087.  elapsed: 0:20:26.\nbatch3,960 of  8087.  elapsed: 0:20:39.\nbatch4,000 of  8087.  elapsed: 0:20:51.\nbatch4,040 of  8087.  elapsed: 0:21:04.\nbatch4,080 of  8087.  elapsed: 0:21:16.\nbatch4,120 of  8087.  elapsed: 0:21:29.\nbatch4,160 of  8087.  elapsed: 0:21:41.\nbatch4,200 of  8087.  elapsed: 0:21:54.\nbatch4,240 of  8087.  elapsed: 0:22:06.\nbatch4,280 of  8087.  elapsed: 0:22:19.\nbatch4,320 of  8087.  elapsed: 0:22:31.\nbatch4,360 of  8087.  elapsed: 0:22:44.\nbatch4,400 of  8087.  elapsed: 0:22:56.\nbatch4,440 of  8087.  elapsed: 0:23:09.\nbatch4,480 of  8087.  elapsed: 0:23:21.\nbatch4,520 of  8087.  elapsed: 0:23:34.\nbatch4,560 of  8087.  elapsed: 0:23:46.\nbatch4,600 of  8087.  elapsed: 0:23:59.\nbatch4,640 of  8087.  elapsed: 0:24:11.\nbatch4,680 of  8087.  elapsed: 0:24:24.\nbatch4,720 of  8087.  elapsed: 0:24:36.\nbatch4,760 of  8087.  elapsed: 0:24:49.\nbatch4,800 of  8087.  elapsed: 0:25:01.\nbatch4,840 of  8087.  elapsed: 0:25:14.\nbatch4,880 of  8087.  elapsed: 0:25:26.\nbatch4,920 of  8087.  elapsed: 0:25:39.\nbatch4,960 of  8087.  elapsed: 0:25:51.\nbatch5,000 of  8087.  elapsed: 0:26:04.\nbatch5,040 of  8087.  elapsed: 0:26:16.\nbatch5,080 of  8087.  elapsed: 0:26:29.\nbatch5,120 of  8087.  elapsed: 0:26:41.\nbatch5,160 of  8087.  elapsed: 0:26:54.\nbatch5,200 of  8087.  elapsed: 0:27:07.\nbatch5,240 of  8087.  elapsed: 0:27:19.\nbatch5,280 of  8087.  elapsed: 0:27:31.\nbatch5,320 of  8087.  elapsed: 0:27:44.\nbatch5,360 of  8087.  elapsed: 0:27:57.\nbatch5,400 of  8087.  elapsed: 0:28:09.\nbatch5,440 of  8087.  elapsed: 0:28:21.\nbatch5,480 of  8087.  elapsed: 0:28:34.\nbatch5,520 of  8087.  elapsed: 0:28:46.\nbatch5,560 of  8087.  elapsed: 0:28:59.\nbatch5,600 of  8087.  elapsed: 0:29:11.\nbatch5,640 of  8087.  elapsed: 0:29:24.\nbatch5,680 of  8087.  elapsed: 0:29:36.\nbatch5,720 of  8087.  elapsed: 0:29:49.\nbatch5,760 of  8087.  elapsed: 0:30:01.\nbatch5,800 of  8087.  elapsed: 0:30:14.\nbatch5,840 of  8087.  elapsed: 0:30:26.\nbatch5,880 of  8087.  elapsed: 0:30:39.\nbatch5,920 of  8087.  elapsed: 0:30:51.\nbatch5,960 of  8087.  elapsed: 0:31:04.\nbatch6,000 of  8087.  elapsed: 0:31:17.\nbatch6,040 of  8087.  elapsed: 0:31:29.\nbatch6,080 of  8087.  elapsed: 0:31:41.\nbatch6,120 of  8087.  elapsed: 0:31:54.\nbatch6,160 of  8087.  elapsed: 0:32:07.\nbatch6,200 of  8087.  elapsed: 0:32:19.\nbatch6,240 of  8087.  elapsed: 0:32:32.\nbatch6,280 of  8087.  elapsed: 0:32:44.\nbatch6,320 of  8087.  elapsed: 0:32:57.\nbatch6,360 of  8087.  elapsed: 0:33:09.\nbatch6,400 of  8087.  elapsed: 0:33:22.\nbatch6,440 of  8087.  elapsed: 0:33:34.\nbatch6,480 of  8087.  elapsed: 0:33:47.\nbatch6,520 of  8087.  elapsed: 0:33:59.\nbatch6,560 of  8087.  elapsed: 0:34:12.\nbatch6,600 of  8087.  elapsed: 0:34:24.\nbatch6,640 of  8087.  elapsed: 0:34:37.\nbatch6,680 of  8087.  elapsed: 0:34:49.\nbatch6,720 of  8087.  elapsed: 0:35:02.\nbatch6,760 of  8087.  elapsed: 0:35:14.\nbatch6,800 of  8087.  elapsed: 0:35:27.\nbatch6,840 of  8087.  elapsed: 0:35:39.\nbatch6,880 of  8087.  elapsed: 0:35:52.\nbatch6,920 of  8087.  elapsed: 0:36:04.\nbatch6,960 of  8087.  elapsed: 0:36:17.\nbatch7,000 of  8087.  elapsed: 0:36:29.\nbatch7,040 of  8087.  elapsed: 0:36:42.\nbatch7,080 of  8087.  elapsed: 0:36:54.\nbatch7,120 of  8087.  elapsed: 0:37:07.\nbatch7,160 of  8087.  elapsed: 0:37:19.\nbatch7,200 of  8087.  elapsed: 0:37:32.\nbatch7,240 of  8087.  elapsed: 0:37:44.\nbatch7,280 of  8087.  elapsed: 0:37:57.\nbatch7,320 of  8087.  elapsed: 0:38:09.\nbatch7,360 of  8087.  elapsed: 0:38:22.\nbatch7,400 of  8087.  elapsed: 0:38:34.\nbatch7,440 of  8087.  elapsed: 0:38:47.\nbatch7,480 of  8087.  elapsed: 0:38:59.\nbatch7,520 of  8087.  elapsed: 0:39:12.\nbatch7,560 of  8087.  elapsed: 0:39:24.\nbatch7,600 of  8087.  elapsed: 0:39:37.\nbatch7,640 of  8087.  elapsed: 0:39:49.\nbatch7,680 of  8087.  elapsed: 0:40:02.\nbatch7,720 of  8087.  elapsed: 0:40:15.\nbatch7,760 of  8087.  elapsed: 0:40:27.\nbatch7,800 of  8087.  elapsed: 0:40:39.\nbatch7,840 of  8087.  elapsed: 0:40:52.\nbatch7,880 of  8087.  elapsed: 0:41:05.\nbatch7,920 of  8087.  elapsed: 0:41:17.\nbatch7,960 of  8087.  elapsed: 0:41:30.\nbatch8,000 of  8087.  elapsed: 0:41:42.\nbatch8,040 of  8087.  elapsed: 0:41:55.\nbatch8,080 of  8087.  elapsed: 0:42:07.\n\naverage training loss:0.19\ntraining epoch took:0:42:09\n\nrunning validation...\naccuracy:0.90\naverage validation loss:0.25\nvalidation took:0:03:10\n\ntraining complete!\ntotal training took 1:30:43 (h:mm:ss)\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"#review the summary of the training process\npd.set_option('precision',2)\ndf_stats=pd.DataFrame(data=training_stats)\ndf_stats=df_stats.set_index('epoch')\ndf_stats","execution_count":18,"outputs":[{"output_type":"execute_result","execution_count":18,"data":{"text/plain":"       training loss  valid.loss  valid accur. training time validation time\nepoch                                                                       \n1               0.31        0.25          0.89       0:42:14         0:03:10\n2               0.19        0.25          0.90       0:42:09         0:03:10","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>training loss</th>\n      <th>valid.loss</th>\n      <th>valid accur.</th>\n      <th>training time</th>\n      <th>validation time</th>\n    </tr>\n    <tr>\n      <th>epoch</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>1</th>\n      <td>0.31</td>\n      <td>0.25</td>\n      <td>0.89</td>\n      <td>0:42:14</td>\n      <td>0:03:10</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.19</td>\n      <td>0.25</td>\n      <td>0.90</td>\n      <td>0:42:09</td>\n      <td>0:03:10</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"#loss plot\nsns.set(style='darkgrid')\nsns.set(font_scale=1.5)\nplt.rcParams[\"figure.figsize\"]=(12,6)\nplt.plot(df_stats[\"training loss\"],'b-o',label=\"Training\")\nplt.plot(df_stats[\"valid.loss\"],'g-o',label=\"Validation\")\n\nplt.title(\"Training and validation loss\")\nplt.xlabel(\"Epoch\")\nplt.ylabel(\"Loss\")\nplt.legend()\nplt.xticks([1,2]) ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### performance on test data"},{"metadata":{"trusted":true},"cell_type":"code","source":"test_data['q1_q2_sentences'] = test_data[['question1','question2']].apply(lambda x:str(x[0])+\" \"+str(x[1]), axis=1)\nsentences=test_data.q1_q2_sentences.values\nlabels=test_data.is_duplicate.values\ninput_ids=[]\nattention_masks=[]\nfor sent in sentences:\n    encoded_dict=tokenizer.encode_plus(\n    sent,\n    add_special_tokens=True,\n    max_length=100, \n    pad_to_max_length=True,\n    return_attention_mask=True,\n    return_tensors='pt') #pytorch tensor\n    \n    input_ids.append(encoded_dict['input_ids'])\n    attention_masks.append(encoded_dict['attention_mask'])\n    \ninput_ids=torch.cat(input_ids,dim=0) \nattention_masks=torch.cat(attention_masks,dim=0)\nlabels=torch.tensor(labels)\n\nbatch_size=32 \n\n#create the dataloader\nprediction_data=TensorDataset(input_ids,attention_masks,labels)\nprediction_sampler = SequentialSampler(prediction_data)\nprediction_dataloader=DataLoader(prediction_data,sampler=prediction_sampler,batch_size=batch_size)","execution_count":19,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#imbalance test dataset\ntest_duplicate = test_data['is_duplicate'].value_counts()\ntest_duplicate / test_duplicate.sum() ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#evaluate on test data\n#prediction on test data\nprint(\"predicting labels for {:,} test data...\".format(len(input_ids)))\nmodel.eval()\n\npredictions,true_labels=[],[]\n\n#predict\nfor batch in prediction_dataloader:\n    batch=tuple(t.to(device) for t in batch)\n    b_input_ids,b_input_mask,b_labels=batch\n        \n    with torch.no_grad():\n        outputs=model(b_input_ids,token_type_ids=None,\n                     attention_mask=b_input_mask) \n    logits=outputs[0]\n    \n    logits=logits.detach().cpu().numpy()\n    label_ids=b_labels.to('cpu').numpy()\n    \n    predictions.append(logits)\n    true_labels.append(label_ids)\nprint('Done')","execution_count":56,"outputs":[{"output_type":"stream","text":"predicting labels for 80,871 test data...\nDone\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"#prediction for this batch are a 2-column array,turn into a list \npred_labels_list=[]\nfor i in range(len(true_labels)):\n    pred_labels_i = np.argmax(predictions[i],axis=1).flatten()\n    pred_labels_list.append(pred_labels_i)","execution_count":21,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pred_list=[]\nfor i in range(2526):\n    for j in range(32):\n        pred_list.append(pred_labels_list[i][j])\ntrue_list=[]\nfor i in range(2526):\n    for j in range(32):\n        true_list.append(true_labels[i][j])\ntrue_array=np.array(true_list)\npred_array=np.array(pred_list)","execution_count":93,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#PR curve\nprec, recall, _ = precision_recall_curve(true_array, pred_array, #y_test(y_true),y_score:\n                                         pos_label=None) \npr_display = PrecisionRecallDisplay(precision=prec, recall=recall).plot()","execution_count":101,"outputs":[{"output_type":"display_data","data":{"text/plain":"<Figure size 432x288 with 1 Axes>","image/png":"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\n"},"metadata":{"needs_background":"light"}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"aps=average_precision_score(true_array, pred_array, average='macro', pos_label=1, sample_weight=None)\nprint(\"Average precision score is:\",aps)","execution_count":102,"outputs":[{"output_type":"stream","text":"Average precision score is: 0.7976728633488381\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"model3_f1 = f1_score(true_array, pred_array)\nmodel3_precision = precision_score(true_array, pred_array) \nmodel3_recall = recall_score(true_array, pred_array) \nmodel3_auc = roc_auc_score(true_array, pred_array)\nprint('Test F1 score for Model 3:', model3_f1, '\\nTest precision for Model 3:', model3_precision, \n      '\\nTest recall for Model 3:', model3_recall,'\\nTest auc for Model 3:', model3_auc)","execution_count":105,"outputs":[{"output_type":"stream","text":"Test F1 score for Model 3: 0.8700851603551368 \nTest precision for Model 3: 0.8507054048830768 \nTest recall for Model 3: 0.890368472507838 \nTest auc for Model 3: 0.8998931420367171\n","name":"stdout"}]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}